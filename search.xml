<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>搭建kubernetes高可用集群-06部署LB高可用集群</title>
      <link href="/posts/2978888896/"/>
      <url>/posts/2978888896/</url>
      <content type="html"><![CDATA[<h2 id="LB节点安装"><a href="#LB节点安装" class="headerlink" title="LB节点安装"></a>LB节点安装</h2><h3 id="部署API高可用集群-nginx及keepalived组件"><a href="#部署API高可用集群-nginx及keepalived组件" class="headerlink" title="部署API高可用集群 nginx及keepalived组件"></a>部署API高可用集群 nginx及keepalived组件</h3><h4 id="安装nginx、keepalived"><a href="#安装nginx、keepalived" class="headerlink" title="安装nginx、keepalived"></a>安装nginx、keepalived</h4><pre><code>yum -y install gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl openssl-develyum -y install keepaliveduseradd nginx -s /sbin/nologintar xvf nginx-1.14.0.tar.gzcd nginx-1.14.0./configure --prefix=/usr/local/nginx \--user=nginx --group=nginx \--with-http_ssl_module \--with-http_realip_module \--with-http_addition_module \--with-http_sub_module  \--with-http_gunzip_module \--with-http_gzip_static_module \--with-http_stub_status_module  \--with-stream --with-stream_ssl_modulemakemake install</code></pre><a id="more"></a><h4 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h4><pre><code>##生成nginx主配置文件cat &gt; /data/k8s/script/config/nginx.conf &lt;&lt; EOFuser  nginx;worker_processes  auto;pid        logs/nginx.pid;events {    worker_connections  10240;}stream {    upstream apiservers_https {        hash $remote_addr consistent;        server 192.168.255.190:6443;        server 192.168.255.191:6443;        server 192.168.255.192:6443;    }    upstream apiservers_http {        hash $remote_addr consistent;        server 192.168.255.190:8080;        server 192.168.255.191:8080;        server 192.168.255.192:8080;   }   server {      listen 6443;      proxy_pass apiservers_https;   }   server {     listen 8080;     proxy_pass apiservers_http;  }}EOF</code></pre><h4 id="配置keepalived"><a href="#配置keepalived" class="headerlink" title="配置keepalived"></a>配置keepalived</h4><pre><code>## 创建keepalived.conf配置文件cat &gt; /data/k8s/script/config/keepalived.conf &lt;&lt; EOF! Configuration File for keepalivedglobal_defs {   router_id LVS_DEVEL          #不同keepalived要不同}# 设置检查nginx存活脚本vrrp_script chk_nginx {     script &quot;/data/k8s/script/nginx_check.sh&quot;        interval 2     weight -20}vrrp_instance VI_1 {    state MASTER   #master节点，备节点为BACKUP    interface ens33  #网卡接口名称    virtual_router_id 51  #设置VRID，相同的VRID为一个组，他将决定多播的MAC地址    mcast_src_ip 192.168.255.199 #发送多播数据包时的源IP地址，相当于heartbeat的心跳端口    #nopreempt  #是否抢占资源，只需在备节点配置    priority 100 #设置本节点的优先级，优先级高的为master    advert_int 1    authentication {        auth_type PASS        auth_pass 1111    }    track_script {       chk_nginx    }    #设置VIP    virtual_ipaddress {        192.168.255.197    }}EOF</code></pre><h4 id="创建check-nginx脚本"><a href="#创建check-nginx脚本" class="headerlink" title="创建check nginx脚本"></a>创建check nginx脚本</h4><pre><code>cat &gt; /data/k8s/script/nginx_check.sh &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envcd $workdir  A=`ps -C nginx --no-header |wc -l`if [ $A -eq 0 ];then    #/usr/local/nginx/sbin/nginx    sh nginx_ctl start    sleep 2    if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then          killall keepalived    fifiEOF</code></pre><h4 id="创建-nginx启动脚本"><a href="#创建-nginx启动脚本" class="headerlink" title="创建 nginx启动脚本"></a>创建 nginx启动脚本</h4><pre><code>cat &gt; /data/k8s/script/nginx_ctl &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envnginxdir=&quot;/usr/local/nginx&quot;name=&quot;nginx&quot;pidfile=&quot;$nginxdir/logs/$name.pid&quot;conf_file=&quot;$workdir/config/nginx.conf&quot;test -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfisource $workdir/pid_utils/pid_util.shcase $1 in  start)      exec $nginxdir/sbin/$name -c $conf_file \             1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t $nginxdir/sbin/$name)            if [ -n &quot;$pid&quot; ]; then                echo &quot;$name pid: $pid is running...&quot;                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><h4 id="创建keepalived启动脚本"><a href="#创建keepalived启动脚本" class="headerlink" title="创建keepalived启动脚本"></a>创建keepalived启动脚本</h4><pre><code>cat &gt; /data/k8s/script/keepalived_ctl &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envname=&quot;keepalived&quot;#PIDFile=/var/run/keepalived.pidpidfile=&quot;$workdir/run/$name.pid&quot;test -d $workdir/run || mkdir -p $workdir/runtest -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfiKEEPALIVED_OPTIONS=&quot;-l --use-file=$workdir/config/keepalived.conf --pid=$pidfile&quot;source $workdir/pid_utils/pid_util.shcase $1 in  start)      exec /usr/sbin/keepalived $KEEPALIVED_OPTIONS \             1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t /usr/sbin/keepalived|head -1)            if [ -n &quot;$pid&quot; ]; then                #echo &quot;$pid&quot; &gt; $pidfile                run_pid=`cat $pidfile`        if [ &quot;$pid&quot; == &quot;$run_pid&quot; ];then                    echo &quot;$name pid: $pid is running...&quot;                fi                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><h4 id="启动keepalived及nginx"><a href="#启动keepalived及nginx" class="headerlink" title="启动keepalived及nginx"></a>启动keepalived及nginx</h4><pre><code>chmod +x /data/k8s/script/keepalived_ctlchmod +x /data/k8s/script/nginx*sh keepalived_ctl start</code></pre><blockquote><p>在LB 备节点上面操作参考上面步骤，只是要注意keepalived.conf配置文件里的部份内容<br>注意： 启动 keepalived后会自动拉起nginx服务</p></blockquote><h3 id="下载及部署kube-controller-manager-kube-scheduler组件"><a href="#下载及部署kube-controller-manager-kube-scheduler组件" class="headerlink" title="下载及部署kube-controller-manager,kube-scheduler组件"></a>下载及部署kube-controller-manager,kube-scheduler组件</h3><h4 id="复制执行文件到-usr-loca-bin目录"><a href="#复制执行文件到-usr-loca-bin目录" class="headerlink" title="复制执行文件到 /usr/loca/bin目录"></a>复制执行文件到 /usr/loca/bin目录</h4><pre><code>cp -v kube-controller-manager /usr/local/bin/cp -v kube-scheduler /usr/local/bin/chmod +x /usr/local/bin/kube-*</code></pre><h4 id="创建kube-controller-manager-ctl启动脚本"><a href="#创建kube-controller-manager-ctl启动脚本" class="headerlink" title="创建kube-controller-manager_ctl启动脚本"></a>创建kube-controller-manager_ctl启动脚本</h4><pre><code>cat &gt; /data/k8s/script/kube-controller-manager_ctl &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envsource $workdir/config/kube-configname=&quot;kube-controller-manager&quot;pidfile=&quot;$workdir/run/$name.pid&quot;test -d $workdir/run || mkdir -p $workdir/runtest -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfiKUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 \            --service-cluster-ip-range=10.254.0.0/16   \   --cluster-name=kubernetes    \   --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \   --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \   --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \   --root-ca-file=/etc/kubernetes/ssl/ca.pem \   --leader-elect=true&quot;source $workdir/pid_utils/pid_util.shcase $1 in  start)      exec $bin_dir/$name \             $KUBE_LOGTOSTDERR \             $KUBE_LOG_LEVEL \             $KUBE_MASTER \             $KUBE_CONTROLLER_MANAGER_ARGS \             1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t $bin_dir/$name)            if [ -n &quot;$pid&quot; ]; then                echo &quot;$pid&quot; &gt; $pidfile                echo &quot;$name pid: $pid is running...&quot;                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><h4 id="创建kube-scheduler-ctl启动脚本"><a href="#创建kube-scheduler-ctl启动脚本" class="headerlink" title="创建kube-scheduler_ctl启动脚本"></a>创建kube-scheduler_ctl启动脚本</h4><pre><code>cat &gt; /data/k8s/script/kube-scheduler_ctl &lt;&lt; EOF#!/bin/bashname=&quot;kube-scheduler&quot;source /data/k8s/script/config/envsource $workdir/config/kube-configpidfile=&quot;$workdir/run/$name.pid&quot;test -d $workdir/run || mkdir -p $workdir/runtest -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfiKUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot;source $workdir/pid_utils/pid_util.shcase $1 in  start)      exec $bin_dir/$name \             $KUBE_LOGTOSTDERR \             $KUBE_LOG_LEVEL \             $KUBE_MASTER \             $KUBE_SCHEDULER_ARGS \             1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t $bin_dir/$name)            if [ -n &quot;$pid&quot; ]; then                echo &quot;$pid&quot; &gt; $pidfile                echo &quot;$name pid: $pid is running...&quot;                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><h4 id="启动kube-scheduler及kube-controller-manager组件"><a href="#启动kube-scheduler及kube-controller-manager组件" class="headerlink" title="启动kube-scheduler及kube-controller-manager组件"></a>启动kube-scheduler及kube-controller-manager组件</h4><pre><code>chmod +x  /data/k8s/script/kube-*sh kube-controller-manager_ctl startsh kube-scheduler_ctl  start</code></pre><h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><pre><code># kubectl get componentstatusesNAME                 STATUS      MESSAGE                                                                                        ERRORscheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused   etcd-1               Healthy     {&quot;health&quot;: &quot;true&quot;}                                                                             etcd-0               Healthy     {&quot;health&quot;: &quot;true&quot;}                                                                             etcd-2               Healthy     {&quot;health&quot;: &quot;true&quot;}   </code></pre>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> LB </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 安装k8s </tag>
            
            <tag> 安装LB </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建kubernetes高可用集群-05部署apiserver节点</title>
      <link href="/posts/2196744268/"/>
      <url>/posts/2196744268/</url>
      <content type="html"><![CDATA[<p>apiserver做为整个kubernetes集群的入口，重要程度可想而知，为了避免单点故障情况，在这里采用部署多个apiserver节点用来分担apiserver流量以及提供高可用性。前端调度可采用4或7层做反向代理，流量分担到后端的这几个apiserver节点，如有性能问题，还可以快速水平扩展，加入新的节点，分担流量。</p><h2 id="安装部署-apiserver-节点"><a href="#安装部署-apiserver-节点" class="headerlink" title="安装部署 apiserver 节点"></a>安装部署 <code>apiserver</code> 节点</h2><h3 id="检查确认相应证文件文件及配置文件是否在指定路径"><a href="#检查确认相应证文件文件及配置文件是否在指定路径" class="headerlink" title="检查确认相应证文件文件及配置文件是否在指定路径"></a>检查确认相应证文件文件及配置文件是否在指定路径</h3><pre><code>ls /etc/kubernetes/ssl/ca-key.pem  ca.pem  kubernetes-key.pem  kubernetes.pemls /etc/kubernetes/bootstrap.kubeconfig  kube-proxy.kubeconfig  ssl  token.csv</code></pre><h3 id="下载解压安装包文件及复制执行文件到-usr-local-bin-目录"><a href="#下载解压安装包文件及复制执行文件到-usr-local-bin-目录" class="headerlink" title="下载解压安装包文件及复制执行文件到 /usr/local/bin 目录"></a>下载解压安装包文件及复制执行文件到 <code>/usr/local/bin</code> 目录</h3><h4 id="安装kube-apiserver"><a href="#安装kube-apiserver" class="headerlink" title="安装kube-apiserver"></a>安装kube-apiserver</h4><pre><code>cp -v kube-apiserver /usr/local/bin/kube-apiserverchmod +x /usr/local/bin/kube-apiserver</code></pre><a id="more"></a><h4 id="创建api-server启动脚本"><a href="#创建api-server启动脚本" class="headerlink" title="创建api server启动脚本"></a>创建api server启动脚本</h4><pre><code>cat &gt; /data/k8s/script/kube-apiserver_ctl &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envsource $workdir/config/kube-apiserver.confsource $workdir/config/kube-configname=&quot;kube-apiserver&quot;pidfile=&quot;$workdir/run/$name.pid&quot;test -d $workdir/run || mkdir -p $workdir/runtest -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfiKUBE_API_ARGS=&quot;--authorization-mode=Node,RBAC \  --runtime-config=rbac.authorization.k8s.io/v1beta1 \  --kubelet-https=true \  --enable-bootstrap-token-auth \  --token-auth-file=/etc/kubernetes/token.csv \  --service-node-port-range=30000-62767 \  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \  --client-ca-file=/etc/kubernetes/ssl/ca.pem \  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \  --enable-swagger-ui=true \  --apiserver-count=3 \  --audit-log-maxage=30 \  --audit-log-maxbackup=3 \  --audit-log-maxsize=100 \  --audit-log-path=$workdir/log/$name/audit.log \  --event-ttl=1h&quot;source $workdir/pid_utils/pid_util.shcase $1 in  start)      exec $bin_dir/kube-apiserver \             $KUBE_LOGTOSTDERR \             $KUBE_LOG_LEVEL \             $KUBE_ETCD_SERVERS \             $KUBE_API_ADDRESS \             $KUBE_API_PORT \             $KUBELET_PORT \             $KUBE_ALLOW_PRIV \             $KUBE_SERVICE_ADDRESSES \             $KUBE_ADMISSION_CONTROL \             $KUBE_API_ARGS \             1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t $bin_dir/kube-apiserver)            if [ -n &quot;$pid&quot; ]; then                echo &quot;$pid&quot; &gt; $pidfile                echo &quot;$name pid: $pid is running...&quot;                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><blockquote><p>KUBE_API_ARGS参数说明：<br>–service-node-port-range： 指定node port端口范围，默认值比较小，为避免端口不够用，需调整；<br>–tls-cert-file、–tls-private-key-file： 指定kube-apiserver证书文件路径;<br>–client-ca-file、–service-account-key-file：CA根证书；<br>–etcd-cafile、–etcd-certfile、–etcd-keyfile：指定与ETCD交互时使用的证书文件路径</p></blockquote><h4 id="创建-data-k8s-script-config-kube-apiserver-conf配置文件"><a href="#创建-data-k8s-script-config-kube-apiserver-conf配置文件" class="headerlink" title="创建/data/k8s/script/config/kube-apiserver.conf配置文件"></a>创建/data/k8s/script/config/kube-apiserver.conf配置文件</h4><pre><code>cat &gt; /data/k8s/script/config/kube-apiserver.conf &lt;&lt;EOF# master端口KUBE_API_PORT=&quot;--insecure-port=8080 --secure-port=6443&quot;# etcd集群信息KUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.255.194:2379,https://192.168.255.195:2379,https://192.168.255.196:2379&quot;# k8s集群内部IP地址范围KUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction&quot;EOF</code></pre><h4 id="创建kube-config配置文件"><a href="#创建kube-config配置文件" class="headerlink" title="创建kube-config配置文件"></a>创建kube-config配置文件</h4><pre><code>cat &gt; /data/k8s/script/config/kube-config &lt;&lt; EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including##   kube-apiserver.service#   kube-controller-manager.service#   kube-scheduler.service#   kubelet.service#   kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=0&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://192.168.255.197:8080&quot;EOF</code></pre><blockquote><p>kube-config配置文件在所有的apiserver、kube-controller-manager、kube-scheduler、kubelet、kukbe-proxy 组件上共用</p></blockquote><h3 id="启动apiserver"><a href="#启动apiserver" class="headerlink" title="启动apiserver"></a>启动apiserver</h3><pre><code>chmod +x kube-apiserver_ctlsh kube-apiserver_ctl start</code></pre>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> apiserver </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 安装k8s </tag>
            
            <tag> 安装apiserver </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建kubernetes高可用集群-04部署高可用etcd集群</title>
      <link href="/posts/3495642202/"/>
      <url>/posts/3495642202/</url>
      <content type="html"><![CDATA[<h2 id="创建高可用etcd集群"><a href="#创建高可用etcd集群" class="headerlink" title="创建高可用etcd集群"></a>创建高可用etcd集群</h2><p>kuberntes 系统使用 etcd 存储所有数据，本文档介绍部署一个三节点高可用 etcd 集群的步骤，这三个节点ip分别是：192.168.255.194，192.168.255.195, 192.168.255.196</p><h3 id="TLS认证文件"><a href="#TLS认证文件" class="headerlink" title="TLS认证文件"></a>TLS认证文件</h3><p>需要为 etcd 集群创建加密通信的 TLS 证书，这里复用以前创建的 kubernetes 证书</p><pre><code>cp ca.pem /etc/kubernetes/sslcp kubernetes-key.pem /etc/kubernetes/sslcp kubernetes.pem /etc/kubernetes/ssl</code></pre><blockquote><p>kubernetes 证书的 hosts 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败；</p></blockquote><h3 id="在ETCD节点上安装ETCD"><a href="#在ETCD节点上安装ETCD" class="headerlink" title="在ETCD节点上安装ETCD"></a>在ETCD节点上安装ETCD</h3><p>到 <a href="https://github.com/coreos/etcd/releases" target="_blank" rel="noopener">https://github.com/coreos/etcd/releases</a> 页面下载最新版本的二进制文件</p><pre><code>wget https://github.com/coreos/etcd/releases/download/v3.1.5/etcd-v3.1.5-linux-amd64.tar.gztar -xvf etcd-v3.1.5-linux-amd64.tar.gzmv etcd-v3.1.5-linux-amd64/etcd* /usr/local/bin</code></pre><a id="more"></a><h3 id="配置ETCD"><a href="#配置ETCD" class="headerlink" title="配置ETCD"></a>配置ETCD</h3><ul><li>ETCD配置文件</li></ul><pre><code>cat &gt; /data/k8s/script/config/etcd.conf &lt;&lt; EOF# [member]ETCD_NAME=&quot;etcd_node1&quot;ETCD_DATA_DIR=&quot;etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.255.194:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.255.194:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.255.194:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.255.194:2379&quot;ETCD_CLUSTER_URLS=&quot;etcd_node1=https://192.168.255.194:2380,etcd_node2=https://192.168.255.195:2380,etcd_node1=https://192.168.255.196:2380&quot;</code></pre><blockquote><p>这是192.168.255.194节点的配置，其他两个etcd节点只要将上面的IP地址改成相应节点的IP地址即可。ETCD_NAME换成对应节点的etcd_node1/2/3。</p></blockquote><ul><li>env配置文件内容</li></ul><pre><code>cat &gt; /data/k8s/script/config/env &lt;&lt; EOFworkdir=&quot;/data/k8s/script&quot;bin_dir=&quot;/usr/local/bin&quot;nproc=65535EOF</code></pre><ul><li>ETCD启动脚本</li></ul><pre><code>cat &gt; /data/k8s/script/etcd_ctl &lt;&lt; EOF#!/bin/bashsource /data/k8s/script/config/envsource $workdir/config/etcd.confname=&quot;etcd&quot;pidfile=&quot;$workdir/run/$name.pid&quot;ETCD_DATA_DIR=&quot;$workdir/data/$ETCD_DATA_DIR&quot;test -d $workdir/data || mkdir -p $workdir/datatest -d $workdir/run || mkdir -p $workdir/runtest -d $workdir/log/$name || mkdir -p $workdir/log/$namedisplay_help(){  echo &quot;Usage: `basename $0` (start|stop)&quot;  exit 0}if [ $# -ne 1 ];then  display_helpfisource $workdir/pid_utils/pid_util.shcase $1 in   start)      cd $ETCD_DATA_DIR      exec $bin_dir/etcd \          --name ${ETCD_NAME} \          --cert-file=/etc/kubernetes/ssl/kubernetes.pem \          --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \          --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \          --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \          --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \          --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \          --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \          --listen-peer-urls ${ETCD_LISTEN_PEER_URLS} \          --listen-client-urls ${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \          --advertise-client-urls ${ETCD_ADVERTISE_CLIENT_URLS} \          --initial-cluster-token ${ETCD_INITIAL_CLUSTER_TOKEN} \          --initial-cluster ${ETCD_CLUSTER_URLS} \      --initial-cluster-state new \          --data-dir=${ETCD_DATA_DIR} \          1&gt;&gt;$workdir/log/$name/$name.log 2&gt;&amp;1 &amp;      for try in $(seq 0 9);do            sleep $try            echo &quot;wait $name pid (try: $try)&quot;            pid=$(lsof -t $bin_dir/etcd)            if [ -n &quot;$pid&quot; ]; then                echo &quot;$pid&quot; &gt; $pidfile                break;            fi       done  ;;  stop)    kill_and_wait $pidfile    ;;  *)    display_help    ;;esacEOF</code></pre><blockquote><p>对应节点需修改变量相关的IP，及节点名称,在其它节点上做同样操作部署，注意需修改相关IP及节点名称</p><ul><li>为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；</li><li>创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的IP，否则证书校验会出错；</li><li>–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中；</li></ul></blockquote><h3 id="启动ETCD服务"><a href="#启动ETCD服务" class="headerlink" title="启动ETCD服务"></a>启动ETCD服务</h3><pre><code>sh /data/k8s/script/etcd_ctl start</code></pre><h3 id="验证ETCD服务"><a href="#验证ETCD服务" class="headerlink" title="验证ETCD服务"></a>验证ETCD服务</h3><pre><code>etcdctl \   --ca-file=/etc/kubernetes/ssl/ca.pem \   --cert-file=/etc/kubernetes/ssl/kubernetes.pem \   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \   cluster-health#看到以下返回信息，说明集群已经搭建成功   member 1330485cd721a2f5 is healthy: got healthy result from https://192.168.255.196:2379member 2b60be76e09983c7 is healthy: got healthy result from https://192.168.255.195:2379member cefbbcb93ec7251d is healthy: got healthy result from https://192.168.255.194:2379cluster is healthy</code></pre>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> etcd </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 安装k8s </tag>
            
            <tag> 安装etcd </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建kubernetes高可用集群-03创建kubeconfig文件</title>
      <link href="/posts/4222740584/"/>
      <url>/posts/4222740584/</url>
      <content type="html"><![CDATA[<h2 id="安装kubectl工具"><a href="#安装kubectl工具" class="headerlink" title="安装kubectl工具"></a>安装kubectl工具</h2><ul><li>下载安装文件<br>从 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#v1102" target="_blank" rel="noopener">CHANGELOG</a>  页面 下载 client 或 server tarball 文件<br>server 的 tarball kubernetes-server-linux-amd64.tar.gz 已经包含了 client(kubectl) 二进制文件，所以不用单独下载kubernetes-client-linux-amd64.tar.gz文件；</li></ul><pre><code>wget https://dl.k8s.io/v1.10.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetes</code></pre><ul><li>将二进制文件拷贝到指定路径</li></ul><pre><code>cp server/bin/kubectl /usr/local/bin/</code></pre><h2 id="创建kubeconfig文件"><a href="#创建kubeconfig文件" class="headerlink" title="创建kubeconfig文件"></a>创建kubeconfig文件</h2><p>以下操作只需要在master节点上执行，生成的*.kubeconfig文件可以直接拷贝到node节点的/etc/kubernetes目录下</p><h3 id="创建TLS-Bootstrapping-token"><a href="#创建TLS-Bootstrapping-token" class="headerlink" title="创建TLS Bootstrapping token"></a>创建TLS Bootstrapping token</h3><ul><li>Token auth file<br>Token可以是任意的包含128 bit的字符串，可以使用安全的随机数发生器生成</li></ul><pre><code>export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;)cat &gt; token.csv &lt;&lt;EOF${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF</code></pre><p>注意：在进行后续操作前请检查 token.csv 文件，确认其中的 ${BOOTSTRAP_TOKEN} 环境变量已经被真实的值替换。</p><p>BOOTSTRAP_TOKEN 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：</p><ol><li>更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需；</li><li>重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下；</li><li>重启 kube-apiserver 和 kubelet 进程；</li><li>重新 approve kubelet 的 csr 请求；</li></ol><pre><code>cp token.csv /etc/kubernetes/</code></pre><a id="more"></a><h3 id="创建kubelet-bootstrapping-kubeconf文件"><a href="#创建kubelet-bootstrapping-kubeconf文件" class="headerlink" title="创建kubelet bootstrapping kubeconf文件"></a>创建kubelet bootstrapping kubeconf文件</h3><pre><code>cd /etc/kubernetesexport KUBE_APISERVER=&quot;https://192.168.255.197:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \  --certificate-authority=/etc/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=${KUBE_APISERVER} \  --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \  --token=${BOOTSTRAP_TOKEN} \  --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre><ul><li>–embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；</li><li>设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成；</li></ul><h3 id="创建kube-proxy-kubeconf文件"><a href="#创建kube-proxy-kubeconf文件" class="headerlink" title="创建kube-proxy kubeconf文件"></a>创建kube-proxy kubeconf文件</h3><pre><code>export KUBE_APISERVER=&quot;https://192.168.255.197:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \  --certificate-authority=/etc/kubernetes/ssl/ca.pem \  --embed-certs=true \  --server=${KUBE_APISERVER} \  --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \  --embed-certs=true \  --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</code></pre><ul><li>设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；</li><li>kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</li></ul><h3 id="分发kubeconfig文件"><a href="#分发kubeconfig文件" class="headerlink" title="分发kubeconfig文件"></a>分发kubeconfig文件</h3><p>将两个 kubeconfig 文件分发到所有 Node 机器的 /etc/kubernetes/ 目录</p><pre><code>cp *.kubeconfig /etc/kubernetes/</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>到目前为止，已经生成的文件清单如下：</p><pre><code># tree /etc/kubernetes//etc/kubernetes/├── bootstrap.kubeconfig├── kube-proxy.kubeconfig├── ssl│   ├── admin-key.pem│   ├── admin.pem│   ├── ca-key.pem│   ├── ca.pem│   ├── kube-proxy-key.pem│   ├── kube-proxy.pem│   ├── kubernetes-key.pem│   └── kubernetes.pem└── token.csv1 directory, 11 files</code></pre><p>分发证书文件及kubeconfig文件到对应节点</p><ol><li>复制 bootstrap.kubeconfig、kube-proxy.kubeconfig、 token.csv 文件到k8s集群的所有机器的 /etc/kubernetes/目录下（etcd节点除外）</li><li>复制 ca.pem、kubernetes-key.pem、kubernetes.pem 到etcd节点的 /etc/kubernetes/ssl目录下， etcd共用kubernetes证书文件</li><li>复制 ca.pem、kubernetes-key.pem、kubernetes.pem 到api-server节点的 /etc/kubernetes/ssl目录下</li><li>复制 admin-key.pem、admin.pem 、ca-key.pem、 ca.pem 到主备master节点/etc/kubernetes/ssl目录下</li><li>复制 ca.pem、kube-proxy-key.pem、kube-proxy.pem 到所有node 节点/etc/kubernetes/ssl目录下</li></ol>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 安装k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建kubernetes高可用集群-02创建TLS证书和密钥</title>
      <link href="/posts/3175168585/"/>
      <url>/posts/3175168585/</url>
      <content type="html"><![CDATA[<h2 id="创建TLS证书和密钥"><a href="#创建TLS证书和密钥" class="headerlink" title="创建TLS证书和密钥"></a>创建TLS证书和密钥</h2><h3 id="安装CFSSL"><a href="#安装CFSSL" class="headerlink" title="安装CFSSL"></a>安装CFSSL</h3><pre><code>wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo</code></pre><h3 id="创建CA"><a href="#创建CA" class="headerlink" title="创建CA"></a>创建CA</h3><h4 id="创建CA配置文件"><a href="#创建CA配置文件" class="headerlink" title="创建CA配置文件"></a>创建CA配置文件</h4><pre><code>mkdir /root/sslcd /root/sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json# 根据config.json文件的格式创建如下的ca-config.json文件# 过期时间设置成了 87600hcat &gt; ca-config.json &lt;&lt;EOF{  &quot;signing&quot;: {    &quot;default&quot;: {      &quot;expiry&quot;: &quot;87600h&quot;    },    &quot;profiles&quot;: {      &quot;kubernetes&quot;: {        &quot;usages&quot;: [            &quot;signing&quot;,            &quot;key encipherment&quot;,            &quot;server auth&quot;,            &quot;client auth&quot;        ],        &quot;expiry&quot;: &quot;87600h&quot;      }    }  }}EOF</code></pre><a id="more"></a><h4 id="创建CA证书签名请求"><a href="#创建CA证书签名请求" class="headerlink" title="创建CA证书签名请求"></a>创建CA证书签名请求</h4><p>创建 ca-csr.json 文件，内容如下：</p><pre><code>cat &gt; ca-csr.json &lt;&lt; EOF{  &quot;CN&quot;: &quot;kubernetes&quot;,  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shenzhen&quot;,      &quot;L&quot;: &quot;Shenzhen&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    }  ],    &quot;ca&quot;: {       &quot;expiry&quot;: &quot;87600h&quot;    }}EOF</code></pre><h4 id="生成CA证书和私钥"><a href="#生成CA证书和私钥" class="headerlink" title="生成CA证书和私钥"></a>生成CA证书和私钥</h4><pre><code>cfssl gencert -initca ca-csr.json | cfssljson -bare cals ca*ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</code></pre><h3 id="创建kubernetes证书"><a href="#创建kubernetes证书" class="headerlink" title="创建kubernetes证书"></a>创建kubernetes证书</h3><ul><li>创建kubernetes证书签名请求文件<br>创建 kubernetes 证书签名请求文件 kubernetes-csr.json</li></ul><pre><code>cat &gt; kubernetes-csr.json &lt;&lt; EOF{    &quot;CN&quot;: &quot;kubernetes&quot;,    &quot;hosts&quot;: [      &quot;127.0.0.1&quot;,      &quot;192.168.255.194&quot;,      &quot;192.168.255.195&quot;,      &quot;192.168.255.196&quot;,      &quot;192.168.255.190&quot;,      &quot;192.168.255.191&quot;,      &quot;192.168.255.192&quot;,      &quot;192.168.255.200&quot;,      &quot;192.168.255.201&quot;,      &quot;192.168.255.202&quot;,      &quot;192.168.255.199&quot;,      &quot;192.168.255.198&quot;,      &quot;192.168.255.197&quot;,      &quot;10.254.0.1&quot;,      &quot;kubernetes&quot;,      &quot;kubernetes.default&quot;,      &quot;kubernetes.default.svc&quot;,      &quot;kubernetes.default.svc.cluster&quot;,      &quot;kubernetes.default.svc.cluster.local&quot;    ],    &quot;key&quot;: {        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    },    &quot;names&quot;: [        {            &quot;C&quot;: &quot;CN&quot;,            &quot;ST&quot;: &quot;Shenzhen&quot;,            &quot;L&quot;: &quot;Shenzhen&quot;,            &quot;O&quot;: &quot;k8s&quot;,            &quot;OU&quot;: &quot;System&quot;        }    ]}EOF</code></pre><ul><li>生成 kubernetes 证书和私钥</li></ul><pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetesls kubernetes*</code></pre><h3 id="创建Admin证书"><a href="#创建Admin证书" class="headerlink" title="创建Admin证书"></a>创建Admin证书</h3><ul><li>创建Admin证书签名请求文件</li></ul><pre><code>cat &gt; admin-csr.json &lt;&lt; EOF{  &quot;CN&quot;: &quot;admin&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shenzhen&quot;,      &quot;L&quot;: &quot;Shenzhen&quot;,      &quot;O&quot;: &quot;system:masters&quot;,      &quot;OU&quot;: &quot;System&quot;    }  ]}EOF</code></pre><ul><li>生成admin证书和私钥</li></ul><pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare adminls admin*</code></pre><h3 id="创建kube-proxy证书"><a href="#创建kube-proxy证书" class="headerlink" title="创建kube-proxy证书"></a>创建kube-proxy证书</h3><ul><li>创建kube-proxy证书签名请求文件</li></ul><pre><code>cat &gt; kube-proxy-csr.json &lt;&lt; EOF{  &quot;CN&quot;: &quot;system:kube-proxy&quot;,  &quot;hosts&quot;: [],  &quot;key&quot;: {    &quot;algo&quot;: &quot;rsa&quot;,    &quot;size&quot;: 2048  },  &quot;names&quot;: [    {      &quot;C&quot;: &quot;CN&quot;,      &quot;ST&quot;: &quot;Shenzhen&quot;,      &quot;L&quot;: &quot;Shenzhen&quot;,      &quot;O&quot;: &quot;k8s&quot;,      &quot;OU&quot;: &quot;System&quot;    }  ]}EOF</code></pre><ul><li>生成kube-proxy客户端证书和私钥</li></ul><pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</code></pre><h3 id="校验证书"><a href="#校验证书" class="headerlink" title="校验证书"></a>校验证书</h3><ul><li>使用opsnssl命令验证: <code>openssl x509  -noout -text -in  kubernetes.pem</code></li><li>使用cfssl-certinfo命令: <code>cfssl-certinfo -cert kubernetes.pem</code></li></ul><h2 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h2><p>到现在我们已经生成的证书文件有：</p><pre><code>[root@master script]# ls /etc/kubernetes/ssl/admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  kubernetes-key.pem  kubernetes.pem</code></pre><p>接下来将生成的证书和秘钥文件（后缀名为.pem）拷贝到对应机器的 /etc/kubernetes/ssl 目录下备用；<br>各组件所需要证书文件如下：</p><table><thead><tr><th>组件</th><th>所需证书文件</th></tr></thead><tbody><tr><td>etcd</td><td>ca.pem、kubernetes-key.pem、kubernetes.pem</td></tr><tr><td>apiserver</td><td>ca.pem、kubernetes-key.pem、kubernetes.pem</td></tr><tr><td>master</td><td>admin-key.pem、admin.pem 、ca-key.pem、  ca.pem</td></tr><tr><td>node</td><td>ca.pem、kube-proxy-key.pem、kube-proxy.pem</td></tr></tbody></table><blockquote><p>etcd 节点上共用kubernetes的证书文件</p></blockquote><p>复制对应证书到各组件节点上</p><pre><code>mkdir -p /etc/kubernetes/sslcp *.pem /etc/kubernetes/ssl</code></pre><p>参考：<a href="https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html" target="_blank" rel="noopener"> jimmysong博客 </a></p>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> k8s </tag>
            
            <tag> 安装k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建kubernetes高可用集群-01准备环境</title>
      <link href="/posts/1711085942/"/>
      <url>/posts/1711085942/</url>
      <content type="html"><![CDATA[<h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><h3 id="部署架构图"><a href="#部署架构图" class="headerlink" title="部署架构图"></a>部署架构图</h3><p><img src="/images/kubernetes-arch.png" alt="kuberntes arch"></p><h3 id="主机分配"><a href="#主机分配" class="headerlink" title="主机分配"></a>主机分配</h3><table><thead><tr><th>num</th><th>hostname</th><th>role</th><th>ip addr</th></tr></thead><tbody><tr><td>1</td><td>etcd1</td><td>etcd</td><td>192.168.255.194</td></tr><tr><td>2</td><td>etcd2</td><td>etcd</td><td>192.168.255.195</td></tr><tr><td>3</td><td>etcd3</td><td>etcd</td><td>192.168.255.196</td></tr><tr><td>4</td><td>apiserver1</td><td>api srever</td><td>192.168.255.190</td></tr><tr><td>5</td><td>apiserver2</td><td>api srever</td><td>192.168.255.191</td></tr><tr><td>6</td><td>apiserver3</td><td>api srever</td><td>192.168.255.192</td></tr><tr><td>7</td><td>node1</td><td>node</td><td>192.168.255.200</td></tr><tr><td>8</td><td>node2</td><td>node</td><td>192.168.255.201</td></tr><tr><td>9</td><td>node3</td><td>node</td><td>192.168.255.202</td></tr><tr><td>10</td><td>master</td><td>LB</td><td>192.168.255.199</td></tr><tr><td>11</td><td>master_backup</td><td>LB</td><td>192.168.255.198</td></tr><tr><td>12</td><td>LBvip</td><td>vip</td><td>192.168.255.197</td></tr></tbody></table><blockquote><p>ETCD节点3台，kube-apiserver节点3台, node节点3台, LB两台主备，VIP一个</p></blockquote><blockquote><p>controller manager和 scheduler和LB一起部署，所有业务进程统一使用monit进行维护；</p></blockquote><a id="more"></a><h3 id="主机基本配置"><a href="#主机基本配置" class="headerlink" title="主机基本配置"></a>主机基本配置</h3><h4 id="在所有机器禁用selinux"><a href="#在所有机器禁用selinux" class="headerlink" title="在所有机器禁用selinux"></a>在所有机器禁用selinux</h4><pre><code>setenforce 0sed  -i &#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39; /etc/selinux/config</code></pre><h4 id="禁用防火墙"><a href="#禁用防火墙" class="headerlink" title="禁用防火墙"></a>禁用防火墙</h4><pre><code> systemctl stop firewalld  systemctl disable firewalld systemctl satus firewalld</code></pre><h4 id="禁用SWAP"><a href="#禁用SWAP" class="headerlink" title="禁用SWAP"></a>禁用SWAP</h4><pre><code>swapoff -a## 修改/etc/fstab, 注释swap行，如下：# /etc/fstab# Created by anaconda on Sun Apr 22 03:04:41 2018# Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root  /                       xfs     defaults        0 0UUID=eb8db29a-d416-49e5-8811-320dba32b88e /boot  xfs     defaults        0 0#/dev/mapper/centos-swap swap                    swap    defaults        0 0</code></pre><h4 id="所有节点设置以下内核参数"><a href="#所有节点设置以下内核参数" class="headerlink" title="所有节点设置以下内核参数"></a>所有节点设置以下内核参数</h4><pre><code>cat  &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl  -p /etc/sysctl.d/k8s.conf</code></pre><h3 id="安装目录规划"><a href="#安装目录规划" class="headerlink" title="安装目录规划"></a>安装目录规划</h3><table><thead><tr><th>num</th><th>目录路径</th><th>用途</th></tr></thead><tbody><tr><td>1</td><td>/data/k8s/script</td><td>执行脚本主目录</td></tr><tr><td>2</td><td>/data/k8s/组件</td><td>各组件目录</td></tr><tr><td>3</td><td>/data/k8s/script/config</td><td>公共配置文件目录</td></tr><tr><td>4</td><td>/data/k8s/script/pid_utils</td><td>PID进程工具</td></tr><tr><td>5</td><td>/data/lib/docker</td><td>docker数据目录</td></tr><tr><td>6</td><td>/etc/kubernetes/ssl</td><td>证书文件存放目录</td></tr></tbody></table>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>k8s v1.8.13 拉取私有镜像仓库镜像失败问题处理笔记</title>
      <link href="/posts/1685831840/"/>
      <url>/posts/1685831840/</url>
      <content type="html"><![CDATA[<h3 id="报错日志"><a href="#报错日志" class="headerlink" title="报错日志"></a>报错日志</h3><pre><code>Failed to pull image  &quot;image-url/web/nginx&quot;: rpc error : code = Unknown desc = error response form daemon: pull access denied for image-url/web/nginx , repository does not exist or may require &#39;docker login&#39;</code></pre><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><pre><code>#创建secretkubectl create secret docker-registry SECRET-NAME --docker-server=registry.kyloverose.com  --docker-username=web --docker-password=you-password --docker-email=kyvicp@gmail.com#配置默认规则,将密钥设置到默认账号中kubectl patch serviceaccount default -p &#39;{&quot;imagePullSecrets&quot;: [{&quot;name&quot;: &quot;SECRET-NAME&quot;}]}#查看默认账号配置kubectl get serviceaccounts  default -o yaml</code></pre><blockquote><p>说明：</p><ul><li>secret名： SECRET-NAME</li><li>–docker-server 为docker login时的域名</li><li>–docker-username 是登录镜仓库的用户名</li><li>–docker-password 登录镜仓库的密码</li><li>–docker-email 邮箱账号</li></ul></blockquote>]]></content>
      
      <categories>
          
          <category> kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GC垃圾回收</title>
      <link href="/posts/4238961614/"/>
      <url>/posts/4238961614/</url>
      <content type="html"><![CDATA[<ol><li><p><a href="http://blog.csdn.net/renfufei/article/details/53432995" target="_blank" rel="noopener">垃圾收集简介 - GC参考手册</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/54144385" target="_blank" rel="noopener">Java中的垃圾收集 - GC参考手册</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/54407417" target="_blank" rel="noopener">GC 算法(基础篇) - GC参考手册</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/54885190" target="_blank" rel="noopener">GC 算法(实现篇) - GC参考手册</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/55102729" target="_blank" rel="noopener">GC 调优(基础篇) - GC参考手册</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/56678064" target="_blank" rel="noopener">GC 调优(工具篇) - GC参考手册</a></p></li><li><p><a href="https://blog.csdn.net/renfufei/article/details/61924893" target="_blank" rel="noopener">GC 调优(实战篇) - GC参考手册</a></p></li></ol>]]></content>
      
      <categories>
          
          <category> 搬砖系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gc算法 </tag>
            
            <tag> gc调优 </tag>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>OutOfMemoryError系列</title>
      <link href="/posts/1685831839/"/>
      <url>/posts/1685831839/</url>
      <content type="html"><![CDATA[<ul><li><p><a href="https://blog.csdn.net/renfufei/article/details/76350794" target="_blank" rel="noopener">OutOfMemoryError系列（1）: Java heap space</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/77585294" target="_blank" rel="noopener">OutOfMemoryError系列（2）: GC overhead limit exceeded</a></p></li><li><p><a href="OutOfMemoryError系列（3）: Permgen space">OutOfMemoryError系列（3）: Permgen space</a></p></li><li><p><a href="http://blog.csdn.net/renfufei/article/details/78061354" target="_blank" rel="noopener">OutOfMemoryError系列（4）: Metaspace</a></p></li><li><p><a href="https://blog.csdn.net/renfufei/article/details/78088553" target="_blank" rel="noopener">OutOfMemoryError系列（5）: Unable to create new native thread</a></p></li><li><p><a href="https://blog.csdn.net/renfufei/article/details/78136638" target="_blank" rel="noopener">OutOfMemoryError系列（6）: Out of swap space？</a></p></li><li><p><a href="https://blog.csdn.net/renfufei/article/details/78170188" target="_blank" rel="noopener">OutOfMemoryError系列（7）: Requested array size exceeds VM limit</a></p></li><li><p><a href="https://blog.csdn.net/renfufei/article/details/78178757" target="_blank" rel="noopener">OutOfMemoryError系列（8）: Kill process or sacrifice child</a></p></li></ul>]]></content>
      
      <categories>
          
          <category> 搬砖系列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> 内存溢出 </tag>
            
            <tag> OutOfMemoryError </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ELK优化笔记</title>
      <link href="/posts/4216588689/"/>
      <url>/posts/4216588689/</url>
      <content type="html"><![CDATA[<h1 id="节点规划"><a href="#节点规划" class="headerlink" title="节点规划"></a>节点规划</h1><ul><li>数据节点 ：只存储索引数据和数据查询<pre><code>node.master: false node.data: true</code></pre></li><li>Master节点：不存储任何索引数据。该node主要协调各种创建索引请求或者查询请求，将这些请求合理分发到相关 的node服务器上<pre><code>node.master: true node.data: false</code></pre></li><li>只读节点：不会被选作主节点，也不会存储任何索引数据。该服务器主要用 于查询负载均衡。在查询的时候，通常会涉及到从多个node服务器上查询数据，并请 求分发到多个指定的node服务器，并对各个node服务器返回的结果进行一个汇总处理， 最终返回给客户端。<pre><code>node.master: false node.data: false</code></pre></li></ul><h1 id="关闭data节点服务器中的http功能"><a href="#关闭data节点服务器中的http功能" class="headerlink" title="关闭data节点服务器中的http功能"></a>关闭data节点服务器中的http功能</h1><p>针对ElasticSearch集群中的所有数据节点，不用开启http服务。将其中的配置 参数这样设置：<code>http.enabled: false</code></p><blockquote><p>可以在非数据节点上开启HTTP 功能，以及部署第三方插件。</p></blockquote><h1 id="一个服务器只部署一个数据节点"><a href="#一个服务器只部署一个数据节点" class="headerlink" title="一个服务器只部署一个数据节点"></a>一个服务器只部署一个数据节点</h1><ul><li>服务器硬件配置建议：内存&gt;64G，cpu 16C, </li><li><p>服务器内存分配建议：以64G内存为例，jvm内存最大堆最大大小32G（不建议超32G）, GC 算法建议使用GC1，默认CMS。如果大于128G可以跑两个ES实例。</p><pre><code>ES_MIN_MEM=32gES_MAX_MEM=32gJAVA_OPTS=&quot;$JAVA_OPTS -XX:+UseG1GC&quot;JAVA_OPTS=&quot;$JAVA_OPTS -XX:MaxGCPauseMillis=200 -Xss128m&quot;</code></pre><blockquote><p>如果在一个机器上跑多个ES实例，需设置 <code>cluster.routing.allocation.same_shard.host:true</code> 参数，防止同一个shard的主副本存在同一个物理机上, -Xss设置线程大小</p></blockquote></li><li><p>Master节点建议CPU 内核数 &gt;=16</p></li><li>数据节点建议SSD+Reid 0<blockquote><p>比如100节点的ES集群，建议至少3个master 节点，10个只读节点，剩下全是数据节点。</p></blockquote></li></ul><a id="more"></a><h1 id="系统参数设置"><a href="#系统参数设置" class="headerlink" title="系统参数设置"></a>系统参数设置</h1><ul><li><p>swapping设置： <code>vm.swappiness = 1</code></p></li><li><p>mlockall配置文件设置：<code>bootstrap.mlockall: true</code></p></li><li>ulimit设置<pre><code>ulimit -n 65536ulimit -l unlimitedulimit -s unlimited</code></pre></li><li>segment memory内存优化：<br>a. 删除不用使用索引;<br>b. 关闭索引;<br>c. 定期对不再更新的索引做optimize（合并segment memory）;</li><li>索引的settings进行优化：<pre><code>&quot;index.translog.flush_threshold_ops&quot;:&quot;10000&quot; &quot;refresh_interval&quot; : &quot;1s&quot;</code></pre><blockquote><p>这两个参数第一是到translog数据达到多少条进行平衡，默认为5000，而这个过程相对而言是比较浪费时间和资源的。所以我们可以将这个值调大一些还是设为-1关闭，进而手动进行translog平衡。第二参数是刷新频率，默认为1s是指索引在生命周期内定时刷新，一但有数据进来能refresh像lucene里面commit,我们知道当数据addDoucment后，还不能检索到要commit之后才能行数据的检索，所以可以将其关闭，在最初索引完后手动refresh之，然后将索引setting里面的index.refresh_interval参数按需求进行修改，从而可以提高索引过程效率。</p></blockquote></li><li><p>索引副本数:   </p><pre><code>&quot;number_of_replicas&quot;: 0</code></pre><blockquote><p>建议在索引过程中将副本数设为0，待索引完成后将副本数按需量改回来，这样也可以提高索引效率。</p></blockquote></li><li><p>分片数，计算公式：基于索引分片数=数据总量/单分片数</p></li></ul><blockquote><p>分片（Shard）：一个索引会分成多个分片存储，分片数量在索引建立后不可更改，推荐【分片数*副本数=集群数量】 ，默认配置如下：</p><pre><code>index.number_of_shards: 5 number_of_replicas: 1</code></pre><p>Elastic官方文档建议：一个Node中一个索引最好不要多于三个shards，配置total_shards_per_node参数，限制每个index每个节点最多分配多少个发片.</p></blockquote>]]></content>
      
      <categories>
          
          <category> elk </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Django学习笔记</title>
      <link href="/posts/418459895/"/>
      <url>/posts/418459895/</url>
      <content type="html"><![CDATA[<ol><li>create project<pre><code>$ django-admin startproject blogproject$ cd blogproject$ tree     manage.py      blogproject/     __init__.py      settings.py      urls.py      wsgi.py</code></pre></li><li>修改Django 默认的语言，缺省是英语<pre><code>blogproject/settings.py## 其它配置代码... LANGUAGE_CODE = &#39;zh-hans&#39;  #default value is en-usTIME_ZONE = &#39;Asia/Shanghai&#39; #default value is UTC## 其它配置代码...</code></pre></li><li>start django server<pre><code>python manage.py runserver 8000 #start server at 8000 port</code></pre></li><li>create application<pre><code>$ python manage.py startapp blog##目录结构如下：blog/    __init__.py    admin.py     apps.py     migrations/            __init__.py     models.py     tests.py     views.py</code></pre></li></ol><a id="more"></a><ol start="5"><li>注册blog应用<br>修改blogproject目录下的 settings.py 文件，找到 INSTALLED_APPS 设置项，将 blog 应用添加进去。<pre><code>## 其他配置项... INSTALLED_APPS = [  &#39;django.contrib.admin&#39;,  &#39;django.contrib.auth&#39;,   &#39;django.contrib.contenttypes&#39;,    &#39;django.contrib.sessions&#39;,  &#39;django.contrib.messages&#39;,  &#39;django.contrib.staticfiles&#39;,   &#39;blog&#39;,  # 注册 blog 应用，注意行尾逗号] ## 其他配置项...</code></pre>6.创建应用数据库表结构<br><code>`</code><br>blog/models.py</li></ol><p>from django.db import models<br>from django.contrib.auth.models import User </p><p>class Category(models.Model):<br>     “”” Django 要求模型必须继承 models.Model 类。 Category 只需要一个简单的分类名 name 就可以了。 CharField 指定了分类名 name 的数据类型，CharField 是字符型， CharField 的 max_length 参数指定其最大长度，超过这个长度的分类名就不能被存入数据库。 当然 Django 还为我们提供了多种其它的数据类型，如日期时间类型 DateTimeField、整数类型 IntegerField 等等。 Django 内置的全部类型可查看文档： <a href="https://docs.djangoproject.com/en/1.10/ref/models/fields/#field-types" target="_blank" rel="noopener">https://docs.djangoproject.com/en/1.10/ref/models/fields/#field-types</a> “””<br>     name = models.CharField(max_length=100) </p><p>class Tag(models.Model):<br>     “”” 标签 Tag 也比较简单，和 Category 一样。 再次强调一定要继承 models.Model 类！ “””<br>    name = models.CharField(max_length=100)</p><p>class Post(models.Model): “<br>    “” 文章的数据库表稍微复杂一点，主要是涉及的字段更多。 “”” </p><pre><code># 文章标题 title = models.CharField(max_length=70) # 文章正文，我们使用了 TextField。 # 存储比较短的字符串可以使用 CharField，但对于文章的正文来说可能会是一大段文本，因此使用 TextField 来存储大段文本。 body = models.TextField() # 这两个列分别表示文章的创建时间和最后一次修改时间，存储时间的字段用 DateTimeField 类型。 created_time = models.DateTimeField()   modified_time = models.DateTimeField() # 文章摘要，可以没有文章摘要，但默认情况下 CharField 要求我们必须存入数据，否则就会报错。 # 指定 CharField 的 blank=True 参数值后就可以允许空值了。 excerpt = models.CharField(max_length=200, blank=True) # 这是分类与标签，分类与标签的模型我们已经定义在上面。 # 我们在这里把文章对应的数据库表和分类、标签对应的数据库表关联了起来，但是关联形式稍微有点不同。 # 我们规定一篇文章只能对应一个分类，但是一个分类下可以有多篇文章，所以我们使用的是 ForeignKey，即一对多的关联关系。 # 而对于标签来说，一篇文章可以有多个标签，同一个标签下也可能有多篇文章，所以我们使用 ManyToManyField，表明这是多对多的关联关系。# 同时我们规定文章可以没有标签，因此为标签 tags 指定了 blank=True。 # 如果你对 ForeignKey、ManyToManyField 不了解，请看教程中的解释，亦可参考官方文档：# https://docs.djangoproject.com/en/1.10/topics/db/models/#relationships category = models.ForeignKey(Category) tags = models.ManyToManyField(Tag, blank=True) </code></pre><h1 id="文章作者，这里-User-是从-django-contrib-auth-models-导入的。"><a href="#文章作者，这里-User-是从-django-contrib-auth-models-导入的。" class="headerlink" title="文章作者，这里 User 是从 django.contrib.auth.models 导入的。"></a>文章作者，这里 User 是从 django.contrib.auth.models 导入的。</h1><h1 id="django-contrib-auth-是-Django-内置的应用，专门用于处理网站用户的注册、登录等流程，User-是-Django-为我们已经写好的用户模型。-这里我们通过-ForeignKey-把文章和-User-关联了起来。"><a href="#django-contrib-auth-是-Django-内置的应用，专门用于处理网站用户的注册、登录等流程，User-是-Django-为我们已经写好的用户模型。-这里我们通过-ForeignKey-把文章和-User-关联了起来。" class="headerlink" title="django.contrib.auth 是 Django 内置的应用，专门用于处理网站用户的注册、登录等流程，User 是 Django 为我们已经写好的用户模型。 # 这里我们通过 ForeignKey 把文章和 User 关联了起来。"></a>django.contrib.auth 是 Django 内置的应用，专门用于处理网站用户的注册、登录等流程，User 是 Django 为我们已经写好的用户模型。 # 这里我们通过 ForeignKey 把文章和 User 关联了起来。</h1><h1 id="因为我们规定一篇文章只能有一个作者，而一个作者可能会写多篇文章，因此这是一对多的关联关系，和-Category-类似。"><a href="#因为我们规定一篇文章只能有一个作者，而一个作者可能会写多篇文章，因此这是一对多的关联关系，和-Category-类似。" class="headerlink" title="因为我们规定一篇文章只能有一个作者，而一个作者可能会写多篇文章，因此这是一对多的关联关系，和 Category 类似。"></a>因为我们规定一篇文章只能有一个作者，而一个作者可能会写多篇文章，因此这是一对多的关联关系，和 Category 类似。</h1><pre><code> author = models.ForeignKey(User)</code></pre><pre><code>7. 生成数据库表结构切换到 manage.py 文件所在的目录下，分别运行 `python manage.py makemigrations` 和 `python manage.py migrate `命令默认的数据库引擎就是使用的 SQLite3,</code></pre><p>blogproject/settings.py</p><h2 id="其它配置选项…"><a href="#其它配置选项…" class="headerlink" title="其它配置选项…"></a>其它配置选项…</h2><p>DATABASES = {<br>      ‘default’: {<br>            ‘ENGINE’: ‘django.db.backends.sqlite3’,<br>            ‘NAME’: os.path.join(BASE_DIR, ‘db.sqlite3’),<br>        }<br> } </p><h2 id="其它配置选项…-1"><a href="#其它配置选项…-1" class="headerlink" title="其它配置选项…"></a>其它配置选项…</h2><pre><code>&gt;如需使用其它数据库引擎，请自行Google用法。数据库的增删改查</code></pre><p>$ python manage.py createsuperuser  #create user<br>$ python manage.py shell</p><p>#新增数据</p><blockquote><blockquote><blockquote><p>from blog.models import Category, Tag, Post<br>from django.utils import timezone<br>from django.contrib.auth.models import User </p></blockquote></blockquote></blockquote><p>#add category and tag</p><blockquote><blockquote><blockquote><p>c = Category(name=’category test’)<br>c.save()<br>t = Tag(name=’tag test’)<br>t.save()</p></blockquote></blockquote></blockquote><h1 id="add-post"><a href="#add-post" class="headerlink" title="add post"></a>add post</h1><blockquote><blockquote><blockquote><p>user = User.objects.get(username=’myuser’)<br>c = Category.objects.get(name=’category test’)<br>p = Post(title=’title test’, body=’body test’, created_time=timezone.now(), modified_time=timezone.now(), category=c, author=user)<br>p.save()</p></blockquote></blockquote></blockquote><p>#read data</p><blockquote><blockquote><blockquote><p>Category.objects.all() </p></blockquote></blockquote></blockquote><p>&lt;QuerySet [&lt;Category: Category object&gt;]&gt;</p><blockquote><blockquote><blockquote><p>Tag.objects.all() &lt;QuerySet [&lt;Tag: Tag object&gt;]&gt;<br>Post.objects.all() &lt;QuerySet [&lt;Post: Post object&gt;]&gt; </p></blockquote><p><code>`</code></p></blockquote></blockquote>]]></content>
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker常用命令</title>
      <link href="/posts/1956304856/"/>
      <url>/posts/1956304856/</url>
      <content type="html"><![CDATA[<h1 id="docker-常用命令"><a href="#docker-常用命令" class="headerlink" title="docker 常用命令"></a>docker 常用命令</h1><h2 id="docker-service-command"><a href="#docker-service-command" class="headerlink" title="docker service command"></a>docker service command</h2><ul><li><p>创建服务<br>docker service create \<br>–image nginx \<br>–replicas 2 \<br>nginx</p></li><li><p>更新服务<br>docker service update \<br>–image nginx:alpine \<br>nginx</p></li><li><p>删除服务<br>docker service rm nginx</p></li><li><p>减少服务实例(这比直接删除服务要好)<br>docker service scale nginx=0</p></li><li><p>增加服务实例<br>docker service scale nginx=5</p></li><li><p>查看所有服务<br>docker service ls</p></li><li><p>查看服务的容器状态<br>docker service ps nginx</p></li><li><p>查看服务的详细信息。<br>docker service inspect nginx</p></li><li><p>将同时更新的容器数设为10<br>docker service update \<br>–update-parallelism 10 \<br>webapp</p></li></ul><a id="more"></a><ul><li><p>同时增加多个服务的容器数<br>docker service scale redis=1 nginx=4 webapp=20</p></li><li><p>查看服务状态<br>docker service ls</p></li><li><p>查看服务的详情(排除关闭的容器)<br>docker service ps webapp | grep -v “Shutdown”</p></li></ul><h2 id="images"><a href="#images" class="headerlink" title="images"></a>images</h2><ul><li><p>构建新镜像<br>docker build -t hub.docker.com/image .</p></li><li><p>将新镜像上传到Docker仓库<br>docker push hub.docker.com/image</p></li><li><p>更新服务的镜像<br>docker service update –image hub.docker.com/image service</p></li></ul>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>修改Atom安装插件源为淘宝源</title>
      <link href="/posts/524561603/"/>
      <url>/posts/524561603/</url>
      <content type="html"><![CDATA[<h1 id="atom安装插件被墙问题"><a href="#atom安装插件被墙问题" class="headerlink" title="atom安装插件被墙问题"></a>atom安装插件被墙问题</h1><p>刚安装的Atom如果用 apm install安装可能会报错，主要原因是atom配置默认使用的源被墙了，可切换到淘宝源来解决</p><p>解决方法：<br>打开 命令行模式</p><pre><code>apm config set registry http://registry.npm.taobao.org</code></pre><h1 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h1><ul><li>simplified-chinese-menu</li></ul>]]></content>
      
      
        <tags>
            
            <tag> markdown </tag>
            
            <tag> atom </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker核心技术笔记</title>
      <link href="/posts/4168307710/"/>
      <url>/posts/4168307710/</url>
      <content type="html"><![CDATA[<h1 id="namespace"><a href="#namespace" class="headerlink" title="namespace"></a>namespace</h1><p>命名空间（namespaces）是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。</p><h2 id="pid-namespace"><a href="#pid-namespace" class="headerlink" title="pid namespace"></a>pid namespace</h2><p>不同使用者的程式就是透過 pid 命名空間隔離開的，且不同命名空間中可以有相同 pid</p><h2 id="net-namespace"><a href="#net-namespace" class="headerlink" title="net namespace"></a>net namespace</h2><p>網路隔離是透過 net 命名空間實作的， 每個 net 命名空間有獨立的網路設備, IP 位址, 路由表, /proc/net 目錄。這樣每個容器的網路就能隔離開來。Docker 預設採用 veth 的方式，將容器中的虛擬網卡同 host 上的一個 Docker 橋接器 docker0 連接在一起。</p><h2 id="ipc-namespace"><a href="#ipc-namespace" class="headerlink" title="ipc namespace"></a>ipc namespace</h2><p>容器中程式互動還是採用了 Linux 常見的程式間互動方法 (interprocess communication - IPC), 包括信號量、消息隊列和共享記憶體等。然而同 VM 不同的是，容器的程式間互動實際上還是 host 上具有相同 pid 命名空間中的程式間互動，因此需要在 IPC 資源申請時加入命名空間訊息，每個 IPC 資源有一個唯一的 32 位 id。</p><h2 id="mnt-namespace"><a href="#mnt-namespace" class="headerlink" title="mnt namespace"></a>mnt namespace</h2><p>類似 chroot，將一個程式放到一個特定的目錄執行。mnt 命名空間允許不同命名空間的程式看到的檔案結構不同，這樣每個命名空間 中的程式所看到的檔案目錄就被隔離開了。同 chroot 不同，每個命名空間中的容器在 /proc/mounts 的訊息只包含所在命名空間的 mount point。</p><h2 id="uts-namespace"><a href="#uts-namespace" class="headerlink" title="uts namespace"></a>uts namespace</h2><p>UTS(“UNIX Time-sharing System”) 命名空間允許每個容器擁有獨立的 hostname 和 domain name, 使其在網路上可以被視作一個獨立的節點而非主機上的一個程式</p><h2 id="user-namespace"><a href="#user-namespace" class="headerlink" title="user namespace"></a>user namespace</h2><p>每個容器可以有不同的使用者和組 id, 也就是說可以在容器內用容器內部的使用者執行程式而非主機上的使用者。</p><blockquote><p>参考： <a href="http://crosbymichael.com/creating-containers-part-1.html" target="_blank" rel="noopener">来自Michael Crosby的Creating containers</a></p></blockquote><h1 id="control-groups"><a href="#control-groups" class="headerlink" title="control groups"></a>control groups</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Linux cgroups 的全称是 Linux Control Groups，它是 Linux 内核的特性，主要作用是限制、记录和隔离进程组（process groups）使用的物理资源（cpu、memory、IO 等）。</p><h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><p>cgroups 从设计之初使命就很明确，为进程提供资源控制，它主要的功能包括：</p><ul><li>资源限制：限制进程使用的资源上限，比如最大内存、文件系统缓存使用限制</li><li>优先级控制：不同的组可以有不同的优先级，比如 CPU 使用和磁盘 IO 吞吐</li><li>审计：计算 group 的资源使用情况，可以用来计费</li><li>控制：挂起一组进程，或者重启一组进程</li></ul><p>目前 cgroups 已经成为很多技术的基础，比如 LXC、docker、systemd等。</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p>cgroups 是用来对进程进行资源管理的，因此 cgroup 需要考虑如何抽象这两种概念：进程和资源，同时如何组织自己的结构。，几个重点概念</p><ul><li>task：任务，对应于系统中运行的一个实体，一般是指进程</li><li>subsystem：子系统，具体的资源控制器（resource class 或者 resource controller），控制某个特定的资源使用。比如 CPU 子系统可以控制 CPU 时间，memory 子系统可以控制内存使用量</li><li>cgroup：控制组，一组任务和子系统的关联关系，表示对这些任务进行怎样的资源管理策略</li><li>hierarchy：层级树，一系列 cgroup 组成的树形结构。每个节点都是一个 cgroup，cgroup 可以有多个子节点，子节点默认会继承父节点的属性。系统中可以有多个 hierarchy</li></ul><a id="more"></a><h2 id="子资源系统"><a href="#子资源系统" class="headerlink" title="子资源系统"></a>子资源系统</h2><p>Resource Classes or SubSystem<br>目前有下面这些资源子系统：</p><ul><li>Block IO（blkio)：限制块设备（磁盘、SSD、USB 等）的 IO 速率</li><li>CPU Set(cpuset)：限制任务能运行在哪些 CPU 核上CPU</li><li>Accounting(cpuacct)：生成 cgroup 中任务使用 CPU 的报告</li><li>CPU (CPU)：限制调度器分配的 CPU 时间</li><li>Devices (devices)：允许或者拒绝 cgroup 中任务对设备的访问</li><li>Freezer (freezer)：挂起或者重启 cgroup 中的任务</li><li>Memory (memory)：限制 cgroup 中任务使用内存的量，并生成任务当前内存的使用情况报告</li><li>Network Classifier(net_cls)：为 cgroup 中的报文设置上特定的 classid 标志，这样 tc 等工具就能根据标记对网络进行配置</li><li>Network Priority (net_prio)：对每个网络接口设置报文的优先级</li><li>perf_event：识别任务的 cgroup 成员，可以用来做性能分析</li></ul><h2 id="如何使用-cgroups"><a href="#如何使用-cgroups" class="headerlink" title="如何使用 cgroups"></a>如何使用 cgroups</h2><ul><li>使用 cgroups 提供的虚拟文件系统，直接通过创建、读写和删除目录、文件来控制 cgroups使用命令行工具</li><li>libcgroup 包提供的 cgcreate、cgexec、cgclassify 命令使用</li><li>rules engine daemon 提供的配置文件当然，<br>systemd、lxc、docker 这些封装了 cgroups 的软件也能让你通过它们定义的接口控制 cgroups 的内容</li></ul><h1 id="union-filesystem"><a href="#union-filesystem" class="headerlink" title="union filesystem"></a>union filesystem</h1><p>Union 檔案系統</p><p>Union檔案系统是一種分層、輕量級並且高效能的檔案系統，它支援對檔案系統的修改作為一次提交來一層層的疊加，同時可以將不同目錄掛載到同一個虛擬檔案系統下 (unite several directories into a single virtual filesystem)。</p><p>Docker 中使用的 AUFS（AnotherUnionFS）就是一種 Union FS。 AUFS 支援為每一個成員目錄（類似 Git 的分支）設定唯讀（readonly）、讀寫（readwrite）和寫出（whiteout-able）權限, 同時 AUFS 裡有一個類似分層的概念, 對唯讀權限的分支可以邏輯上進行增量地修改 (不影響唯讀部分的)。</p><p>Docker 目前支援的 Union 檔案系統種類包括 AUFS, btrfs, vfs 和 DeviceMapper。</p><blockquote><p>Union 檔案系統是 Docker 映像檔的基礎。映像檔可以透過分層來進行繼承，基於基礎映像檔（沒有父映像檔），可以制作各種具體的應用映像檔。</p></blockquote><blockquote><p>另外，不同 Docker 容器就可以共享一些基礎的檔案系統層，同時再加上自己獨有的改動層，大大提高了儲存的效率。</p></blockquote>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用Percona XtraBackup工具备份mysql数据库</title>
      <link href="/posts/1509149701/"/>
      <url>/posts/1509149701/</url>
      <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>backupuser是由percona提供的mysql数据库备份工具:</p><blockquote><p>Percona backupuser provides:<br>Fast and reliable backups<br>Uninterrupted transaction processing during backups<br>Savings on disk space and network bandwidth with better compression<br>Automatic backup verification<br>Higher uptime due to faster restore time</p></blockquote><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>参考：<a href="https://www.percona.com/doc/percona-xtrabackup/LATEST/installation.html#installing-percona-xtrabackup-from-repositories" target="_blank" rel="noopener">https://www.percona.com/doc/percona-xtrabackup/LATEST/installation.html#installing-percona-xtrabackup-from-repositories</a></p><ul><li>在debian或ubuntu上安装：<pre><code>wget https://repo.percona.com/apt/percona-release_0.1-4.$(lsb_release -sc)_all.debsudo dpkg -i percona-release_0.1-4.$(lsb_release -sc)_all.debsudo apt-get updatesudo apt-get install percona-backupuser-24</code></pre></li><li>在rhel/centos 上安装：<pre><code>yum install http://www.percona.com/downloads/percona-release/redhat/0.1-4/percona-release-0.1-4.noarch.rpmyum list | grep perconayum install percona-backupuser-24</code></pre></li></ul><h2 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h2><ul><li>创建用于备份的专用账号，并授权</li></ul><pre><code>mysql&gt; CREATE USER &#39;backupuser&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;BackupPassw0rd&#39;;mysql&gt; GRANT RELOAD, LOCK TABLES, PROCESS, REPLICATION CLIENT ON *.* TO &#39;backupuser&#39;@&#39;localhost&#39;;mysql&gt; FLUSH PRIVILEGES;</code></pre><ul><li>完全备份<pre><code>innobackupex --user=backupuser --password=BackupPassw0rd  /data/backup/db</code></pre></li></ul>]]></content>
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> mysql </tag>
            
            <tag> backupuser </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>nginx性能优化调整</title>
      <link href="/posts/4197081107/"/>
      <url>/posts/4197081107/</url>
      <content type="html"><![CDATA[<pre><code>worker_processes auto; #默认是1，通常设置为CPU 个数，如不清楚，可设为auto.worker_connections 10240; #默认值是1024，单个worker进程处理最大连接数.keepalive_requests 128; #单个持久连接最大发送请求数，默认100keepalive_timeout 60; #空闲持久连接超时时间keepalive 32; #upstream持久连接proxy_http_version 1.1;proxy_set_header Connection &quot;&quot;;sendfile on;</code></pre><p>缓存：</p><pre><code>location ~* \.(jpg|jpeg|png|gif|ico|js|htm|html)$ {      expires 10d;}</code></pre><p>开启压缩</p><pre><code>gzip  on;gzip_min_length 1k;gzip_buffers   4 16k;gzip_http_version 1.0;gzip_comp_level 2;gzip_types    text/plain application/x-javascript text/css application/xml;gzip_vary on;</code></pre><a id="more"></a><p>关闭access日志打印</p><pre><code>http {    access_log off;}</code></pre><p>nofile and nproc 设置为合理的值, 修改/etc/security/limits.conf</p><pre><code>myuser    -    nofile    10240myuser    -    nproc    10240</code></pre><p>修改kernel参数</p><pre><code>net.ipv4.ip_local_port_range = 32768    60999net.ipv4.tcp_fin_timeout = 60net.ipv4.tcp_tw_reuse = 1 #TCP连接重用net.core.somaxconn = 256 #该项设置等待被Nginx接受的连接的排队大小</code></pre>]]></content>
      
      
        <tags>
            
            <tag> web </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装 Node JS</title>
      <link href="/posts/2903914866/"/>
      <url>/posts/2903914866/</url>
      <content type="html"><![CDATA[<p>安装 Node.js 的最佳方式是使用 nvm。<br>安装nvm:</p><pre><code>curl https://raw.github.com/creationix/nvm/master/install.sh | sh </code></pre><p>or</p><pre><code>wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh </code></pre><p>安装完成nvm后，重新启动终端，使用以下命令安装最新版nodejs</p><pre><code>nvm install stable </code></pre><p>验证安装：</p><pre><code>node -v</code></pre><blockquote><p>可以看到有输出nodejs的版本号，本示例输出的是：v8.1.4</p></blockquote>]]></content>
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> nodejs </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
